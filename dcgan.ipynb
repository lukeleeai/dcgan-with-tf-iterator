{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN with MNIST\n",
    "\n",
    "* MNIST data를 가지고 **Deep Convolution GAN**를 `tf.contrib.slim`을 이용하여 만들어보자.\n",
    "  * [참고: TensorFlow slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shatapy/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#tf.set_random_seed(219)\n",
    "#np.random.seed(219)\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "train_dir = 'train/dcgan/exp1/'\n",
    "max_epochs = 30\n",
    "save_epochs = 10\n",
    "summary_steps = 2500\n",
    "print_steps = 500\n",
    "batch_size = 64\n",
    "learning_rate_D = 0.0002\n",
    "learning_rate_G = 0.001\n",
    "k = 1 # the number of step of learning D before learning G\n",
    "num_samples = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), _ = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`\n",
    "\n",
    "### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (?, 28, 28), types: tf.float64>\n"
     ]
    }
   ],
   "source": [
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count=max_epochs)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conv2d_transpose` => fractional-strided convolution to increase the size of generated images. \n",
    "\n",
    "`padding='SAME' => OUTPUT_HEIGHT = INPUT_HEIGHT / STRIDE  \n",
    "conv2d_transpose & padding='SAME' => OUTPUT_HEIGHT = INPUT_HEIGHT * STRIDE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\"Directly applying batchnorm to all layers\n",
    "however, resulted in sample oscillation and model instability. This was avoided by not applying\n",
    "batchnorm to the generator output layer and the discriminator input layer.\"</strong> from DCGAN paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\"The first layer of the GAN, which takes a uniform noise distribution Z as input, could be called\n",
    "    fully connected as it is just a matrix multiplication, but the result is reshaped into a 4-dimensional\n",
    "    tensor and used as the start of the convolution stack.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "  \"\"\"Deep Convolutional Generative Adversarial Networks\n",
    "  implementation based on http://arxiv.org/abs/1511.06434\n",
    "  \n",
    "  \"Unsupervised Representation Learning with\n",
    "  Deep Convolutional Generative Adversarial Networks\"\n",
    "  Alec Radford, Luke Metz and Soumith Chintala\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, mode, train_dataset, test_dataset=None):\n",
    "    \"\"\"Basic setup.\n",
    "    \n",
    "    Args:\n",
    "      mode (`string`): \"train\" or \"generate\".\n",
    "      train_dataset (`tf.data.Dataset`): train_dataset.\n",
    "      test_dataset (`tf.data.Dataset`): test_dataset.\n",
    "    \"\"\"\n",
    "    assert mode in [\"train\", \"generate\"]\n",
    "    self.mode = mode\n",
    "    \n",
    "    # hyper-parameters for model\n",
    "    self.x_dim = 28\n",
    "    self.z_dim = 100\n",
    "    self.batch_size = batch_size\n",
    "    self.num_samples = num_samples\n",
    "    self.train_dataset = train_dataset\n",
    "    self.test_dataset = test_dataset\n",
    "    \n",
    "    # Global step Tensor.\n",
    "    self.global_step = None\n",
    "    \n",
    "    print('The mode is %s.' % self.mode)\n",
    "    print('complete initializing model.')\n",
    "    \n",
    "    \n",
    "  def build_random_z_inputs(self):\n",
    "    \"\"\"Build a vector random_z in latent space.\n",
    "    \n",
    "    Returns:\n",
    "      self.random_z (`2-rank Tensor` with [batch_size, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "      self.sample_random_z (`2-rank Tensor` with [num_samples, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    " \n",
    "    \"The first layer of the GAN, which takes a uniform noise distribution Z as input, could be called\n",
    "    fully connected as it is just a matrix multiplication, but the result is reshaped into a 4-dimensional\n",
    "    tensor and used as the start of the convolution stack.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope('random_z'):\n",
    "      self.random_z = tf.random_uniform(shape=[self.batch_size, 1, 1, self.z_dim],  \n",
    "                                        minval=-1.0, maxval=1.0)\n",
    "      self.sample_random_z = tf.random_uniform(shape=[self.num_samples, 1, 1, self.z_dim],\n",
    "                                               minval=-1.0, maxval=1.0)\n",
    "\n",
    "    return self.random_z, self.sample_random_z\n",
    "  \n",
    "  \n",
    "  def read_MNIST(self, dataset):\n",
    "    \"\"\"Read MNIST dataset\n",
    "    \n",
    "    Args:\n",
    "      dataset (`tf.data.Dataset` format): MNIST dataset.\n",
    "      \n",
    "    Returns:\n",
    "      self.mnist (`4-rank Tensor` with [batch, x_dim, x_dim, 1]): MNIST dataset with batch size.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('mnist'):\n",
    "      iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "      self.mnist = iterator.get_next()\n",
    "      self.mnist = tf.cast(self.mnist, dtype = tf.float32)\n",
    "      self.mnist = tf.expand_dims(self.mnist, axis=3)\n",
    "      \n",
    "    return self.mnist\n",
    "\n",
    "\n",
    "  def Generator(self, random_z, is_training=True, reuse=False):\n",
    "    \"\"\"Generator setup.\n",
    "    \n",
    "    Args:\n",
    "      random_z (`2-rank Tensor` with [batch_size, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "      is_training (`bool`): whether training mode or test mode.\n",
    "      reuse (`bool`): whether variable reuse or not.\n",
    "      \n",
    "    Returns:\n",
    "      generated_data (`4-rank Tensor` with [batch_size, h, w, c])\n",
    "          generated images from random vector z.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Generator', reuse=reuse) as scope:\n",
    "      batch_norm_params = {'decay': 0.9,\n",
    "                           'epsilon': 0.001,\n",
    "                           'is_training': is_training,\n",
    "                           'scope': 'batch_norm'}\n",
    "      with slim.arg_scope([slim.conv2d_transpose],\n",
    "                          kernel_size=[4, 4],\n",
    "                          stride=[2, 2],\n",
    "                          normalizer_fn=slim.batch_norm,\n",
    "                          normalizer_params=batch_norm_params):\n",
    "        # 여기를 채워 넣으세요\n",
    "        # Use full conv2d_transpose instead of projection and reshape\n",
    "        # conv2d_transpose = fractional-strided convolution to increase the size of generated images. \n",
    "        # random_z: 1 x 1 x 100 dim\n",
    "        self.inputs = random_z\n",
    "        # inputs = random_z: 1 x 1 x 100 dim\n",
    "        # outputs: 3 x 3 x 256 dim\n",
    "        # OH = stride * (IH - 1) + FH - 2 * Padding\n",
    "        self.layer1 = slim.conv2d_transpose(self.inputs, \n",
    "                                            num_outputs=256,\n",
    "                                            kernel_size=[3, 3], \n",
    "                                            padding='VALID')  \n",
    "        # inputs: 3 x 3 x 256 dim\n",
    "        # outputs: 7 x 7 x 128 dim\n",
    "        self.layer2 = slim.conv2d_transpose(self.layer1,\n",
    "                                           num_outputs=128,\n",
    "                                           kernel_size=[3, 3],\n",
    "                                           padding='VALID')\n",
    "        # inputs: 7 x 7 x 128 dim\n",
    "        # outputs: 14 x 14 x 64 dim\n",
    "        self.layer3 = slim.conv2d_transpose(self.layer2,\n",
    "                                           num_outputs=64)\n",
    "        \n",
    "        # inputs: 14 x 14 x 64 dim\n",
    "        # outputs: 28 x 28 x 1 dim\n",
    "        self.layer4 = slim.conv2d_transpose(self.layer3,\n",
    "                                           num_outputs=1,\n",
    "                                           normalizer_fn=None,\n",
    "                                           activation_fn=tf.tanh)\n",
    "        # generated_data = outputs: 28 x 28 x 1 dim\n",
    "        generated_data = self.layer4\n",
    "\n",
    "        return generated_data\n",
    "    \n",
    "    \n",
    "  def Discriminator(self, data, reuse=False):\n",
    "    \"\"\"Discriminator setup.\n",
    "    \n",
    "    Args:\n",
    "      data (`2-rank Tensor` with [batch_size, x_dim]): MNIST real data.\n",
    "      reuse (`bool`): whether variable reuse or not.\n",
    "      \n",
    "    Returns:\n",
    "      logits (`1-rank Tensor` with [batch_size]): logits of data.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Discriminator', reuse=reuse) as scope:\n",
    "      batch_norm_params = {'decay': 0.9,\n",
    "                           'epsilon': 0.001,\n",
    "                           'scope': 'batch_norm'}\n",
    "      with slim.arg_scope([slim.conv2d],\n",
    "                          kernel_size=[4, 4],\n",
    "                          stride=[2, 2],\n",
    "                          activation_fn=tf.nn.leaky_relu,\n",
    "                          normalizer_fn=slim.batch_norm,\n",
    "                          normalizer_params=batch_norm_params):\n",
    "        # 여기를 채워 넣으세요\n",
    "        # inputs = data: 28 x 28 x 1 dim\n",
    "        # outputs: 14 x 14 x 64 dim\n",
    "        self.layer1 = slim.conv2d(data, 64, normalizer_fn=None)\n",
    "        # inputs: 14 x 14 x 64 dim\n",
    "        # outputs: 7 x 7 x 128 dim\n",
    "        self.layer2 = slim.conv2d(self.layer1, 128)\n",
    "        # inputs: 7 x 7 x 128 dim\n",
    "        # outputs: 3 x 3 x 256 dim\n",
    "        # self.layer3 = slim.conv2d(self.layer2, 256)\n",
    "        self.layer3 = slim.conv2d(self.layer2, 256, kernel_size=[3, 3], padding='VALID')\n",
    "        # inputs: 3 x 3 x 256 dim\n",
    "        # outputs: 1 x 1 x 1 dim\n",
    "        self.layer4 = slim.conv2d(self.layer3, 1, kernel_size=[3, 3], \n",
    "                                  stride=[1, 1], \n",
    "                                  padding='VALID',\n",
    "                                  normalizer_fn=None,\n",
    "                                  activation_fn=None)\n",
    "        # logits = layer4: 1 x 1 x 1 dim -> 1 dim\n",
    "        discriminator_logits = tf.squeeze(self.layer4, axis=[1,2])\n",
    "\n",
    "        return discriminator_logits\n",
    "    \n",
    "    \n",
    "  def setup_global_step(self):\n",
    "    \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "    if self.mode == \"train\":\n",
    "      self.global_step = tf.train.get_or_create_global_step()\n",
    "      \n",
    "      print('complete setup global_step.')\n",
    "      \n",
    "      \n",
    "  def GANLoss(self, logits, is_real=True, scope=None):\n",
    "    \"\"\"Computes standard GAN loss between `logits` and `labels`.\n",
    "    \n",
    "    Args:\n",
    "      logits (`1-rank Tensor`): logits.\n",
    "      is_real (`bool`): True means `1` labeling, False means `0` labeling.\n",
    "      \n",
    "    Returns:\n",
    "      loss (`0-randk Tensor): the standard GAN loss value. (binary_cross_entropy)\n",
    "    \"\"\"\n",
    "    if is_real:\n",
    "      labels = tf.ones_like(logits)\n",
    "    else:\n",
    "      labels = tf.zeros_like(logits)\n",
    "\n",
    "    # 여기를 채워 넣으세요\n",
    "    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels,\n",
    "                                          logits=logits, scope=scope)\n",
    "    #sigmoid x_ent = mostly for binary classification.\n",
    "\n",
    "    return loss\n",
    "\n",
    "      \n",
    "  def build(self):\n",
    "    \"\"\"Creates all ops for training or generate.\"\"\"\n",
    "    self.setup_global_step()\n",
    "    \n",
    "    if self.mode == \"generate\":\n",
    "      pass\n",
    "    \n",
    "    else:\n",
    "      # generating random vector\n",
    "      self.random_z, self.sample_random_z = self.build_random_z_inputs()\n",
    "      # read dataset\n",
    "      self.real_data = self.read_MNIST(self.train_dataset)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # generating images from Generator() via random vector z\n",
    "      self.generated_data = self.Generator(self.random_z)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # discriminating real data by Discriminator()\n",
    "      self.real_logits = self.Discriminator(self.real_data)\n",
    "      # discriminating fake data (generated)_images) by Discriminator()\n",
    "      self.fake_logits = self.Discriminator(self.generated_data, reuse=True)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # losses of real with label \"1\"\n",
    "      self.loss_real = self.GANLoss(self.real_logits)\n",
    "      # losses of fake with label \"0\"\n",
    "      self.loss_fake = self.GANLoss(self.fake_logits, is_real=False)\n",
    "      \n",
    "      # losses of Discriminator\n",
    "      with tf.variable_scope('loss_D'):\n",
    "        self.loss_Discriminator = self.loss_real + self.loss_fake\n",
    "        \n",
    "      # 여기를 채워 넣으세요\n",
    "      # losses of Generator with label \"1\" that used to fool the Discriminator\n",
    "      self.loss_Generator = self.GANLoss(self.fake_logits)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # Separate variables for each function\n",
    "      self.D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator')\n",
    "      self.G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n",
    "      \n",
    "      \n",
    "      # generating images for sample\n",
    "      self.sample_data = self.Generator(self.sample_random_z, is_training=False, reuse=True)\n",
    "      \n",
    "      # write summaries\n",
    "      # Add loss summaries\n",
    "      tf.summary.scalar('losses/loss_Discriminator', self.loss_Discriminator)\n",
    "      tf.summary.scalar('losses/loss_Generator', self.loss_Generator)\n",
    "      \n",
    "      # Add histogram summaries\n",
    "      for var in self.D_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      for var in self.G_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      \n",
    "      # Add image summaries\n",
    "      tf.summary.image('random_images', self.generated_data, max_outputs=4)\n",
    "      tf.summary.image('real_images', self.real_data)\n",
    "      \n",
    "    print('complete model build.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_sample_data(sample_data, max_print=num_samples):\n",
    "  print_images = sample_data[:max_print,:]\n",
    "  print_images = print_images.reshape([max_print, 28, 28])\n",
    "  print_images = print_images.swapaxes(0, 1)\n",
    "  print_images = print_images.reshape([28, max_print * 28])\n",
    "    \n",
    "  print(\"printing\")\n",
    "  \n",
    "  plt.figure(figsize=(max_print, 1))\n",
    "  plt.axis('off')\n",
    "  plt.imshow(print_images, cmap='gray')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mode is train.\n",
      "complete initializing model.\n",
      "complete setup global_step.\n",
      "complete model build.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DCGAN(mode=\"train\", train_dataset=train_dataset)\n",
    "model.build()\n",
    "\n",
    "# # show info for trainable variables\n",
    "# t_vars = tf.trainable_variables()\n",
    "# slim.model_analyzer.analyze_vars(t_vars, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_D = tf.train.AdamOptimizer(learning_rate=learning_rate_D, beta1=0.5)\n",
    "opt_G = tf.train.AdamOptimizer(learning_rate=learning_rate_G, beta1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_collection = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Discriminator')\n",
    "G_collection = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Generator')\n",
    "\n",
    "with tf.control_dependencies(D_collection):\n",
    "  opt_D_op = opt_D.minimize(model.loss_Discriminator, var_list=model.D_vars)\n",
    "with tf.control_dependencies(G_collection):\n",
    "  opt_G_op = opt_G.minimize(model.loss_Generator, global_step=model.global_step, var_list=model.G_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign `tf.summary.FileWriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving graph to: train/dcgan/exp1/\n"
     ]
    }
   ],
   "source": [
    "graph_location = train_dir\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.train.Saver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5705b503021d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPUOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1307\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  tf.logging.info('Start Session.')\n",
    "  \n",
    "  num_examples = len(train_data)\n",
    "  num_batches_per_epoch = int(num_examples / batch_size)\n",
    "  \n",
    "  # save loss values for plot\n",
    "  loss_history = []\n",
    "  pre_epochs = 0\n",
    "  while True:\n",
    "    try:\n",
    "      start_time = time.time()\n",
    "      \n",
    "      for _ in range(k):\n",
    "        _, loss_D = sess.run([opt_D_op, model.loss_Discriminator])\n",
    "      _, global_step_, loss_G = sess.run([opt_G_op,\n",
    "                                          model.global_step,\n",
    "                                          model.loss_Generator])\n",
    "      \n",
    "      epochs = global_step_ * batch_size / float(num_examples)\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      if global_step_ % print_steps == 0:\n",
    "        examples_per_sec = batch_size / float(duration)\n",
    "        print(\"Epochs: {:.2f} global_step: {} loss_D: {:.3f} loss_G: {:.3f} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                  epochs, global_step_, loss_D, loss_G, examples_per_sec, duration))\n",
    "\n",
    "        loss_history.append([epochs, loss_D, loss_G])\n",
    "\n",
    "        # print sample data\n",
    "        sample_data = sess.run(model.sample_data)\n",
    "        print_sample_data(sample_data)\n",
    "\n",
    "      # write summaries periodically\n",
    "      if global_step_ % summary_steps == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        train_writer.add_summary(summary_str, global_step=global_step_)\n",
    "\n",
    "      # save model checkpoint periodically\n",
    "      if int(epochs) % save_epochs == 0  and  pre_epochs != int(epochs):\n",
    "        tf.logging.info('Saving model with global step {} (= {} epochs) to disk.'.format(global_step_, int(epochs)))\n",
    "        saver.save(sess, train_dir + 'model.ckpt', global_step=global_step_)\n",
    "        pre_epochs = int(epochs)\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      tf.logging.info('Saving model with global step {} (= {} epochs) to disk.'.format(global_step_, int(epochs)))\n",
    "      saver.save(sess, train_dir + 'model.ckpt', global_step=global_step_)\n",
    "      break\n",
    "      \n",
    "  tf.logging.info('complete training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_history = np.asarray(loss_history)\n",
    "\n",
    "plt.plot(loss_history[:,0], loss_history[:,1], label='loss_D')\n",
    "plt.plot(loss_history[:,0], loss_history[:,2], label='loss_G')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
